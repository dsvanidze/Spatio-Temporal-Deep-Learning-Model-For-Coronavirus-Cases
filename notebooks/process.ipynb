{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a083293-1cdc-4a3a-b8d0-a4a2a6bc3d9d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Have you ever faced the challenge of running your code on different computers, only to find that it doesn't work as expected? Or have you found yourself struggling to keep track of changes you made to your code, wishing you had a better way to collaborate with your team? Then what comes next might bring you some practical advices to help you tackle these challenges. The learning process behind our work _Making deep learning algorithms reproducible: the devil is in the details_ has given us valuable insights into how to ensure the reproducibility of scientific work. This text is linked to two other documents: (a) Jupyter notebook that introduces the problem and the data and allows user to run all code chunks and visualise the results [add link] and (b) detailed instructions (readme file) to reproduce the work [add link]. Throughout this text we will provide links to the notebook so that you can associate the concepts described qualitatively here with practical coding examples. Eventually, this should offer you a complete picture of the main processes required to make deep learning algorithms applied to your own work reproducible. We are glad to share the story of Davit, a master student from Goettingen University, who embarked us into a journey towards making his master thesis reproducible. We hope his story will offer not only useful insights for your work, but also a captivating and enlightening read.\n",
    "\n",
    "The focus of my master thesis was to apply deep learning algorithms models on spatial data to better understand the initial spread of Covid-19 in China. Initially, I gathered all data and started working on my computer. After I built the algorithms to train the data---adapting deep learning algorithms to spatial data was pretty challenging too but that's another story---my first challenge to reproducibility was computational. I realised that training models on my local computer was taking far too long so I needed a faster solution to be able to submit my thesis in time. I had not much choice but to train the models on more powerful computers. Hopefully I had the opportunity to access the university server to train the algorithms. I generated the results on my local computer since producing maps and tables was not as demanding as training deep learning algorithms.\n",
    "\n",
    "But I soon encountered another issue. The [paths](https://en.wikipedia.org/wiki/Path_(computing)) associated with the location of the algorithms were hardcoded. As my code became longer, I overlooked the path names linked to algorithms that was generating the results. This mistake, which would have been very easily to correct if pointed it our earlier, resulted in incorrect results. This error, as minor as it may sound, is unacceptable in science where results may have enormous implications especially in areas where decisions can have important impact on human lives such as public health. And wrong results do not necessarily appear wrong to an audience, especially if the results do not diverge from the findings of the literature. This is where I realised that my code is a fundamental pillar of all my empirical work. How can someone trust my work if not able to verify it?\n",
    "\n",
    "So what was the solution to these initial problems? One might think it would be easy to copy the code from one computer to another, but it turned out to be a real headache. Different operating systems on my local computer and the university server caused many compatibility issues and errors. That's where Docker containers came in handy, allowing me to create an environment where everything needed to run the code was installed, and integrate it on different hardware.\n",
    "\n",
    "But even with Docker, I still faced another challenge: editing code in a convenient way. Jupyter Notebooks solved this by enabling me to edit code in the browser and access it from anywhere. And with the ability to set a seed number for the DL algorithms, I could ensure that the results were consistent and reproducible, regardless of where or when they were run.\n",
    "\n",
    "But it doesn't stop there. I even made our Jupyter Notebook accessible via a link [add link here], so readers can access all the work directly in their browser and run the project in seconds, without having to install anything on their own computer. My goal was to make the verification process as simple as possible, so that my supervisors can appreciate my work and trust the results without having to rely on luck.\n",
    "\n",
    "The problem I faced is representative of a much global issue that has caused a lot of concern in the scientific community. It is often referred to as \"reproducibility crisis\" in science [citations 4,5]. Researchers need to be able to reproduce and replicate findings to make sure that scientific progress is actually happening [citations 1-3]. A study found that around one third of studies published in top journals like *Nature* and *Science* between 2010 and 2015 couldn't be replicated [citation 1]. In other words one cannot be sure about the findings of about 1 third of studies published in two of the most important academic journals. \n",
    "\n",
    "It's important to understand what \"replicability\" and \"reproducibility\" mean in this context. According to the US National Science Foundation (2015) [add link here of the citation], replicability means being able to duplicate the results of a study by following the same procedures but using new data. Reproducibility, on the other hand, means duplicating the results using the same materials as the original investigator. Here we will focus on reproducibility. You might think that getting identical results would be easy for studies that use standard statistical procedures and a given dataset. Indeed, it is sometimes straightforward. But it is a lot harder for studies using AI algorithms, especially in deep learning applied to geographic data, which was the focus of my master thesis. Below you will learn how you you can ensure that your code and work can be reproduced even when you deal with very complex algorithms. Keep reading to find out how!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13a17e-f668-4550-92dc-6aad4e9f3ac8",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "When I first started the project, I had everything set up on my local computer. My goal was to train models and analyze geographic data. However, I soon realized that training models on my local computer was taking an excessive amount of time. This is when I decided to move the training process to the university server, while still generating results on my local computer for better visualization.\n",
    "\n",
    "During this process, I encountered a mistake that could have had a significant impact on my results. The paths to my DL models were hardcoded in my code, and as the code became longer, I overlooked the need to change the path to the correct model when generating results. This simple oversight resulted in incorrect results, which I was lucky enough to only have a minimal impact on. But, this made me realize that code is the foundation for all written work, including results, discussion, and conclusion. How can anyone trust the validity of my work without looking at the base code? This highlights the importance of having clean, organized code, especially for black-box models like DL where even a small mistake can completely change the validity of the results and conclusions.\n",
    "\n",
    "To make sure that the code was easily accessible for myself and the team, I started using Git and Github. This made it easy for us to quickly copy the code to our local machines and contribute to the project. However, having the code wasn't enough to run it and see results. The university server was running Ubuntu, which didn't support the code editor, Python programming language, DL and all other packages in the same way as my MacOS computer did. This caused many compatibility issues and was very time consuming.\n",
    "\n",
    "To solve this, I turned to Docker containers, which created an environment where everything needed to run the code was installed, and I could integrate it on different hardware and use the computation power of that hardware. This made it easy for anyone to reproduce the project on their local computers or servers.\n",
    "\n",
    "Finally, Jupyter Notebooks allowed me to edit the code in a browser, making it accessible from anywhere. This solved many problems, but I also observed that models' results on my server and local computer were different due to the random behavior in DL algorithms and statistical packages. To overcome this, I set a seed number to have the same stream of random values in the DL algorithms, regardless of where or when the results were reproduced.\n",
    "\n",
    "To make the verification process as simple as possible for our readers, we even made our Jupyter Notebook accessible via a link, so one doesn't even need to install anything on their local computer. This allows anyone to access and run our project directly from their browser, making it simple for them to reproduce and trust our work. These tools are available for free and can be used by anyone who is interested in making their own code and work easier to reproduce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95553e60-3816-4c46-a4f2-4b5119e071e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Version Control with Git and Github\n",
    "When I started the project, I was working on everything locally. However, I quickly realized that I needed a way to keep track of changes to my code and collaborate with others without the risk of losing any work. This is where version control comes in.\n",
    "\n",
    "Git is a version control system that allows you to keep track of changes to your code over time. Github is a web-based platform that provides a central repository for storing and sharing code. With Git and Github, I was able to version my code, collaborate with others, and avoid the risk of losing any work. \n",
    "\n",
    "Git and Github are also great for reproducibility. By sharing your code, others can access your work, verify it and reproduce your results. This makes it easy for others to build on your work, or for you to share your results with the wider community. The ability to easily store and share your code also makes it easier to keep track of the different versions of your code and to see how your work has evolved over time.\n",
    "\n",
    "Using Git and Github, I was able to manage my code in a much more organized and efficient way. With version control, I could easily switch between different versions of my code and collaborate with others on the project. And because everything was stored on Github, I never had to worry about losing any work or overwriting important changes.\n",
    "\n",
    "Git and Github made it easy for me to work on my project and collaborate with others without any worry. With version control, I was able to keep my code organized, track changes over time, and ensure that all my work was saved and secure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82104c9d-da08-475b-bd64-dc814311d42c",
   "metadata": {},
   "source": [
    "# Operating System Compatibility Issues\n",
    "After having solved the version control part, everything worked well on my local computer. But when I tried to run my code on the university server, I quickly realized that not all operating systems are created equal. The university server was running Ubuntu, a Linux distribution, which wasn't as compatible with my MacOS-based code editor, Python programming language, and all the deep learning packages I was using. This caused a lot of headaches and took up a lot of my time fixing compatibility issues.\n",
    "\n",
    "But, there's always a solution to every problem. That's where Docker containers came in as a lifesaver. Docker containers allow you to create a virtual environment with all the necessary packages and dependencies installed. This way, you can run your code on any hardware, no matter what operating system is installed, without having to worry about compatibility issues. And, if someone else wants to reproduce your work, they can easily access the container and run your code without having to install anything on their own machine.\n",
    "\n",
    "To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. Here's the link to the Docker [installation guide for MacOS](https://docs.docker.com/docker-for-mac/install/) and for [Ubuntu](https://docs.docker.com/engine/install/ubuntu/).\n",
    "\n",
    "I found the Docker website to be very helpful, with a lot of resources and tutorials available. Once I had Docker installed, it was very easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues.\n",
    "\n",
    "So, not only did Docker containers save me a lot of time and effort, but they also made it easier for others to reproduce my work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ff925-2269-4173-a116-43e8fbef2418",
   "metadata": {},
   "source": [
    "# Editing Code with Jupyter Notebooks\n",
    "As someone who loves coding and exploring new ways to improve my workflow, I've found Jupyter Notebooks to be incredibly convenient for editing my code. Not only does it allow me to keep my code, markdown text, and outputs all in one place, but it also makes it easy for me to experiment and iterate on my code.\n",
    "\n",
    "One of the challenges with machine learning and deep learning (DL) algorithms is that their results can be influenced by randomness. This means that even if I run the same code multiple times, I may get different results. While this is not necessarily a bad thing, it can make it difficult for me to reproduce my results and understand what's driving my findings.\n",
    "\n",
    "In the code below we first set the seed number to 0 using np.random.seed(0). Then, we generate two random arrays arr1 and arr2 using np.random.rand(3,2).\n",
    "\n",
    "As you can see, if we run the code multiple times, the values of arr1 and arr2 will be the same every time. This is because we set the seed number to 0, which ensures that the random number generator produces the same sequence of numbers each time the code is run.\n",
    "\n",
    "```{python}\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate random array\n",
    "arr1 = np.random.rand(3,2)\n",
    "print(\"Array 1:\")\n",
    "print(arr1)\n",
    "\n",
    "# Generate another random array\n",
    "arr2 = np.random.rand(3,2)\n",
    "print(\"\\nArray 2:\")\n",
    "print(arr2)\n",
    "```\n",
    "\n",
    "Now, let's see what happens if we remove the line np.random.seed(0):\n",
    "```{python2}\n",
    "import numpy as np\n",
    "\n",
    "# Generate random array\n",
    "arr1 = np.random.rand(3,2)\n",
    "print(\"Array 1:\")\n",
    "print(arr1)\n",
    "\n",
    "# Generate another random array\n",
    "arr2 = np.random.rand(3,2)\n",
    "print(\"\\nArray 2:\")\n",
    "print(arr2)\n",
    "```\n",
    "\n",
    "As you can see, if we run the code multiple times, the values of arr1 and arr2 will be different each time, as the seed number is not set. This demonstrates how the seed number affects the randomness in DL algorithms and how it is important to set the seed number to ensure replicability.\n",
    "\n",
    "That's why I always make sure to set a seed number when working with these algorithms. This ensures that I get the same random values every time I run my code, making it much easier for me to replicate my results. I also find it helpful to document the seed number I used in my code so that I can easily reproduce my findings in the future.\n",
    "\n",
    "By using Jupyter Notebooks and setting seed numbers, I can make my DL research much more reliable and reproducible. This makes it easier for me to collaborate with other researchers and share my findings with others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09aa04c-f297-4bbc-b295-2545365da71c",
   "metadata": {},
   "source": [
    "# Making the Work Accessible and Reproducible\n",
    "As a data scientist, I understand the importance of making my work accessible and reproducible. That's why I often use Jupyter Notebooks for my projects. Jupyter Notebooks are a powerful tool for organizing and sharing my code, results, and visualizations all in one place. With the ability to mix markdown text, code, and visualizations, I can create a complete narrative of my work that is easy to understand and follow.\n",
    "\n",
    "One of the biggest benefits of using Jupyter Notebooks is their accessibility. By hosting my notebooks on a cloud-based platform like GitHub, I can easily share my work with anyone by providing a simple link. This makes it easy for others to see exactly what I did, how I did it, and what my results were. It also makes it easy for others to reproduce my work, which is crucial for verification and collaboration.\n",
    "\n",
    "Another key advantage of using Jupyter Notebooks is their ability to simplify the verification process. When I share my work, I want to make sure that the verification process is as simple as possible. Jupyter Notebooks make this easy because they allow me to organize my work in a clear and concise manner. By having all of my code, results, and visualizations in one place, anyone can quickly understand what I did and why.\n",
    "\n",
    "Finally, I want to emphasize that Jupyter Notebooks are available for anyone to use, for free. This is a huge advantage because it means that anyone can start using them, regardless of their technical skills or budget. Whether you're a beginner or an experienced data scientist, Jupyter Notebooks can help you make your work accessible and reproducible.\n",
    "\n",
    "So, why is Jupyter Notebooks convenient in this context? Let me give you a few examples.\n",
    "\n",
    "For starters, Jupyter Notebooks allow me to experiment and iterate quickly. Because I can mix text, code, and visualizations, I can easily try different approaches to solving a problem and see the results in real-time. This allows me to test different ideas and get feedback from others without having to spend a lot of time writing and debugging code.\n",
    "\n",
    "Another reason why Jupyter Notebooks are convenient is their ability to handle large datasets. With Jupyter Notebooks, I can load and visualize large datasets right in the notebook. This makes it easy for me to see patterns and correlations in my data, which helps me make better decisions and find insights that I might have missed otherwise.\n",
    "\n",
    "Finally, Jupyter Notebooks are great for collaboration. By sharing my notebooks with others, I can get feedback and collaborate on projects in real-time. This makes it easy for us to work together, regardless of where we are or what time it is. With Jupyter Notebooks, we can all see what each other is doing and make changes together in real-time.\n",
    "\n",
    "In conclusion, Jupyter Notebooks are a convenient and powerful tool for making work accessible and reproducible. By allowing me to mix text, code, and visualizations, Jupyter Notebooks make it easy for me to share my work and collaborate with others. And best of all, Jupyter Notebooks are available for anyone to use, for free, which makes them an ideal tool for making work accessible and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c89b78-c724-4fb7-aa3b-c3edd27529d7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I hope you've enjoyed learning about the importance of reproducibility and trustworthiness in Deep Learning with me. In this article, I've discussed several key concepts and tools that are crucial in making sure your results are reliable, accessible, and verifiable.\n",
    "\n",
    "To summarize, we've talked about:\n",
    "\n",
    "- The need for version control with Git and Github, which allows you to keep track of changes in your code and collaborate with others efficiently\n",
    "Operating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment\n",
    "- The convenience of Jupyter Notebooks for code editing, which is particularly useful for data science and DL work because of its ability to include text and code in the same document\n",
    "The significance of seed numbers to ensure replicability, especially in DL algorithms that rely on randomness\n",
    "- The importance of making the work accessible and reproducible, which can be achieved by using Jupyter Notebooks and providing a clear, simple verification process.\n",
    "\n",
    "To remedy all barriers to reproducibility, we have implemented various elements in a system (illustrated in Figure 1). It has four main components (A - D). We use (A) the version control system Git and its hosting service GitHub, which enable a team to share code with peers, efficiently track and synchronize code changes between local and server machines, and reset the project to a working state in case of breaking changes.\n",
    "\n",
    "![Figure 1](../data/workflow/docker-workflow.jpg \"Figure 1\")\n",
    "\n",
    "\n",
    "Now, let's take a moment to reflect on why reproducibility and trustworthiness are so crucial in DL. In this field, AI models can be used to make decisions that affect people's lives, such as medical diagnoses, financial predictions, and criminal justice assessments. In such cases, it's essential that the results are reliable, and the methods are verifiable. Otherwise, it's possible to make errors that could have serious consequences.\n",
    "\n",
    "On the other hand, reproducibility is also crucial in research. By making your work accessible and verifiable, you can build on the efforts of others and advance the field more quickly. It also helps to avoid duplication of effort and saves time and resources.\n",
    "\n",
    "In conclusion, I want to emphasize that reproducibility and trustworthiness are key principles that must be upheld in DL. With the right tools and processes in place, it's possible to ensure that your work is accessible, reliable, and verifiable. Whether you're working in a research environment or in an industry setting, these principles are essential for advancing the field and building trust in AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30c1c9-6dd8-41e0-8d01-2efe40e019b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
