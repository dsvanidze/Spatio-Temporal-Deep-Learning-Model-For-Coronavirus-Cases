{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58581716-7c64-4de6-94c2-4a6a911e48f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- <figure style=\"width: 30%\">\n",
    "  <img src=\"i/DL1_text_2.png\" style=\"width: 30%\"/>\n",
    "  <figcaption>Credit: Discord software - Midjourney bot</figcaption>\n",
    "</figure>\n",
    "<figure style=\"width: 30%\">\n",
    "  <img src=\"i/DL1_text_1.png\" style=\"width: 30%\"/>\n",
    "  <figcaption>Credit: Discord software - Midjourney bot</figcaption>\n",
    "</figure>\n",
    "<figure style=\"width: 30%\">\n",
    "  <img src=\"i/DL1_text.png\" style=\"width: 30%\"/>\n",
    "  <figcaption>Credit: Discord software - Midjourney bot</figcaption>\n",
    "</figure>\n",
    " -->\n",
    "\n",
    "<div style=\"text-align: center; margin-top: 40px;\">\n",
    "  <img src=\"i/DL1_text_1.png\" style=\"width: 32%; height: auto; margin: 10px;\"/>\n",
    "  <img src=\"i/DL1_text_2.png\" style=\"width: 32%; height: auto; margin: 10px;\"/>\n",
    "  <img src=\"i/DL1_text.png\" style=\"width: 32%; height: auto; margin:10px;\"/>\n",
    "</div>\n",
    "\n",
    " Credit: Discord software - Midjourney bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc6d7d-4667-4fa3-b1e4-62c0e36766cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Making deep learning algorithms reproducible: the devil is in the details\n",
    "<div style=\"float: left; width: 400px; height: 240px; margin: 10px 40px 40px 0\">\n",
    "  <table style=\"margin: 0 auto;\">\n",
    "    <tr>\n",
    "      <td></td>\n",
    "      <td style=\"width: 100px; height: 100px; background-color: black; color: white; text-align: center; font-size: 20px; font-weight: bold;\">1. Jupyter Notebook: Code</td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"width: 100px; height: 100px; background-color: black; color: white; text-align: center; font-size: 20px; font-weight: bold;\">2. GitHub readme: Instructions</td>\n",
    "      <td></td>\n",
    "      <td style=\"width: 100px; height: 100px; background-color: black; color: white; text-align: center; font-size: 20px; font-weight: bold;\">3. Jupyter Notebook: Story</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "## A tripartite structure \n",
    "Have you ever faced the challenge of running your code on different computers, only to find that it doesn't work as expected? Or have you found yourself struggling to keep track of changes you made to your code, wishing you had a better way to collaborate with your team? Then what comes next might bring you some practical pieces of advice to help you tackle these challenges. From the experience acquired through our academic project _Making deep learning algorithms reproducible: the devil is in the details_, we would like to share with you some insights into how to (try to) think about and implement reproducibility for your work.\n",
    "\n",
    "To get a full picture of our work and the process behind it, we propose a tripartite structure composed of [1. Jupyter Notebook: Code](http://c100-159.cloud.gwdg.de:9009/lab/tree/notebooks/main.ipynb?token=7cf55c2887d81e8ea8da627112d0753e4b4fc79345f121fc) that introduces the problem and the data and allows users to run all code chunks and visualize the results; and [2. GitHub readme: Instructions](https://github.com/dsvanidze/replicability#making-deep-learning-algorithms-reproducible-the-devil-is-in-the-details) which are a series of instructions to reproduce the work using a computer with most operating systems (OS). This third component of the structure (3. Jupyter Notebook: Story) offers practical coding examples and illustrations to prepare you to tackle various challenges that you are likely to encounter on your path towards reproducibility. These three acts should hopefully give you a broad picture of the thought process and the technical implementation you might need to make your work reproducible.\n",
    "\n",
    "While one cannot generalize the process to make work reproducible for every single work, the principles of reproducibility are usually the same independent of your research question and goals. Reproducibility can be especially difficult when you use very complex such as deep learning algorithms, but no worries, we will guide you to overcome the main barriers you might encounter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1824cbd-044b-407f-affe-7260eac43518",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Davit's journey towards reproducibility\n",
    "\n",
    "<figure style=\"float: right; width: 350px; height: 350px; margin: 10px 0 56px 40px;\">\n",
    "  <img src=\"i/3computer.png\"/>\n",
    "  <figcaption>Credit: Discord software - Midjourney bot</figcaption>\n",
    "</figure>\n",
    "\n",
    "The main protagonist Davit, who is currently pursuing a Master's degree in Economics at the London School of Economics (LSE), will take us on a journey to make his bachelor thesis reproducible. We hope that his story will offer not only useful insights for your work but also a captivating and enlightening read. Davit's journey will cover the use of Jupyter notebooks, GitHub, and other relevant tools and techniques to ensure that his bachelor thesis is reproducible. He will walk us through the process of documenting his code, data, and results, as well as the importance of using version control systems like Git. Furthermore, Davit will highlight the challenges he faced and how he overcame them while making his thesis reproducible. We will also explore best practices for collaboration, peer review, and dissemination of results.\n",
    "\n",
    "Our goal is to provide an engaging illustrated guide to making research reproducible. By following Davit's journey, you will learn how to organize your work, make your code and data accessible, and document your process. You will also see how to use GitHub to share your work with others, collaborate on projects, and track changes to your code and data over time. In addition, the text will cover various aspects of reproducibility such as transparency, accessibility, and reliability.\n",
    "\n",
    "> <big> Join us on this journey to learn how to make your research reproducible and open for others to review, build upon, and replicate. With the skills and knowledge you gain from Davit's story to apply to your work, you will be able to overcome the challenges of reproducibility and make your work more impactful and trustworthy. </big>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a083293-1cdc-4a3a-b8d0-a4a2a6bc3d9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reproducibility barriers and tips to tackle them\n",
    "\n",
    "<figure style=\"float: left; width: 425px; height: 425px; margin: 10px 40px 56px 0;\">\n",
    "  <img src=\"i/students.png\"/>\n",
    "  <figcaption>Credit: Discord software - Midjourney bot</figcaption>\n",
    "</figure>\n",
    "\n",
    "### More power, please!\n",
    "\n",
    "The focus of my bachelor thesis was to apply deep learning algorithms models on spatial data to better understand the initial spread of Covid-19 in China. Initially, I gathered all data and started working on my computer. After I built the algorithms to train the data — adapting deep learning algorithms to spatial data was pretty challenging too but that's another story — my first challenge to reproducibility was computational. I realized that training models on my local computer were taking far too long so I needed a faster solution to be able to submit my thesis in time. I had not much choice but to train the models on more powerful computers. Fortunately, I could access the university server to train the algorithms. I generated the results on my local computer since producing maps and tables were not as demanding as training deep learning algorithms.\n",
    "\n",
    "\n",
    "### Bloody paths\n",
    "But I soon encountered another issue. The [paths](https://en.wikipedia.org/wiki/Path_(computing)) associated with the location of the algorithms were hard coded. As my code became longer, I overlooked the path names linked to algorithms that were generating the results. This mistake, which would have been very easily corrected if pointed out earlier, resulted in incorrect results. This error, as minor, as it may sound, is unacceptable in science where results may have enormous implications, especially in areas where decisions can have an important impact on human lives such as public health. And wrong results do not necessarily appear wrong to an audience, especially if the results do not diverge from the findings of the literature. This is where I realized that my code is the fundamental pillar of the validity of my empirical work. How can someone trust my work if not able to verify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3f68e-e4d6-4c00-953f-c26f0fb74157",
   "metadata": {},
   "source": [
    "<p style=\"color: orange;\">Hardcoding a path in Python may lead to issues when the code is run in a different environment, as the path might not exist in the new environment. Here is a short code that demonstrates this issue:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714079db-044c-459f-af6a-afc42f2b807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿toy,price\n",
      "A,1\n",
      "B,1.5\n",
      "C,2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded path\n",
    "file_path = \"/all/notebooks/toydata.csv\"\n",
    "try:\n",
    "    with open(file_path) as file:\n",
    "        data = file.read()\n",
    "        print(data)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aac3c3-4ef6-4486-a42a-1000fecf752c",
   "metadata": {},
   "source": [
    "<p style=\"color: orange;\">When run in a different environment, the code would result in a _File not found_ error because the path specified does not exist in the new environment. A better code that uses relative paths can be written as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5b46ae-0062-4b5a-ad8e-0a8bacc203cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿toy,price\n",
      "A,1\n",
      "B,1.5\n",
      "C,2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Relative path\n",
    "import os\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"toydata.csv\")\n",
    "try:\n",
    "    with open(file_path) as file:\n",
    "        data = file.read()\n",
    "        print(data)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0155e-3465-4748-bd93-4ef16b5c987a",
   "metadata": {},
   "source": [
    "<p style=\"color: orange;\">You can see that the code has imported data from the dataset `toydata.csv` and printed its two columns (toy and price) and 3 rows.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3263cc0-220a-42e7-a612-8a2f39e2e549",
   "metadata": {},
   "source": [
    "<br>\n",
    "New example!!!:\n",
    "\n",
    "Hard coding paths (and not only) may seem unproblematic even if you have several models to train, but it can lead to a lot of confusion and problems, which unfortunately happened to me. Here is the code (much shorter and simpler than in real cases) that trains models, stores the results and uses them in the next step to accept/reject a model when compared to the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2740b94e-3871-426d-a735-302568979a0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Cell will be hidden\n",
    "import pandas as pd\n",
    "\n",
    "def train(model=None):\n",
    "    #many things happen\n",
    "    if model == \"simple\":\n",
    "        return pd.DataFrame({\"result\": 30}, index=[0])\n",
    "    elif model == \"complex\":\n",
    "        return pd.DataFrame({\"result\": 70}, index=[0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae53f181-a895-4778-ba31-8f692c3517b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an arbitrary benchmark for simplicity and accept/reject models depending on results vs. benchmark\n",
    "benchmark = 50\n",
    "# Set the model details in one place for a better overview\n",
    "model = {\n",
    "    \"model1\": {\"name\": \"model1\", \"type\": \"simple\"}, \n",
    "    \"model2\": {\"name\": \"model2\", \"type\": \"complex\"}\n",
    "}\n",
    "\n",
    "# Set the current model to \"model1\" to use it for training and check its results\n",
    "current_model = model[\"model1\"]\n",
    "# Train a simple model for \"model1\" and a complex model for \"model2\"\n",
    "# Training result of the \"model1\" is 30 and for \"model2\" is 70\n",
    "model_structure = train(current_model[\"type\"])\n",
    "# Save the model and its result in a .csv file\n",
    "model_structure.to_csv('/all/notebooks/results-of-model1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fffd1cee-8e34-4fad-9c6c-de2716306c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: model1\n",
      "Model type: simple\n",
      "Result: 70\n",
      "\u001b[3;32m>>> Result is better than the benchmark -> Accept the model and use it for calculations\n"
     ]
    }
   ],
   "source": [
    "# Load the model result and compare with the benchmark\n",
    "\n",
    "print(\"Model name: {}\".format(current_model[\"name\"]))\n",
    "print(\"Model type: {}\".format(current_model[\"type\"]))\n",
    "# Load the result of the current model\n",
    "result = pd.read_csv('/all/notebooks/results-of-model2.csv').iloc[0, 0]\n",
    "print(\"Result: {}\".format(result))\n",
    "\n",
    "if result > benchmark:\n",
    "    print(\"\\033[3;32m>>> Result is better than the benchmark -> Accept the model and use it for calculations\")\n",
    "else:\n",
    "    print(\"\\033[3;31m>>> Result is NOT better than the benchmark -> Reject the model as it is not optimal\")\n",
    "    \n",
    "# Some following calculations with the optimal model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3161b-d734-4dc7-aaf3-8c2952184e35",
   "metadata": {},
   "source": [
    "Everything looks fine, doesn't it? Certainly not! Can you see the problem right away? When I tried to train my models and go back and forth, saving them and loading the results, I had to change the hard coded paths each time. First I trained the \"model2\", a complex model, and tested it against the benchmark (70 > 50 -> accepted). I did the same for the \"model1\" (a simple model). Its result was identical to the complex model (\"model2\"), and I chose it because it was simpler and used much less computational resources.\n",
    "\n",
    "However, in line 6, where I load the result for the current model, I forgot to change the path to the path of \"model1\". So I mistakenly loaded the result of \"model2\" thinking it was from \"model1\". I accepted a model which should be rejected by the benchmark (30 < 50) and performed much worse than another model (30 < 70). I then used \"model1\" for all other subsequent calculations, charts, and maps. \n",
    "\n",
    "Such a simple error can can cause minor or critical changes in results. Below is the improved code for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69dc97f-3c4f-4cd8-83bc-39181f842481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an arbitrary benchmark for simplicity and accept/reject models depending on results vs. benchmark\n",
    "benchmark = 50\n",
    "# Set the model details (INCLUDING PATHS) in one place for a better overview\n",
    "model = {\n",
    "    \"model1\": {\"name\": \"model1\", \"type\": \"simple\", \"path\": \"/all/notebooks/results-of-model1.csv\"}, \n",
    "    \"model2\": {\"name\": \"model2\", \"type\": \"complex\", \"path\": \"/all/notebooks/results-of-model2.csv\"}\n",
    "}\n",
    "\n",
    "# Set the current model to \"model1\" to use it for training and check its results\n",
    "current_model = model[\"model1\"]\n",
    "# Train a simple model for \"model1\" and a complex model for \"model2\"\n",
    "# Training result of the \"model1\" is 30 and for \"model2\" is 70\n",
    "model_structure = train(current_model[\"type\"])\n",
    "# Save the model and its result in a .csv file\n",
    "model_structure.to_csv(current_model[\"path\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b339d282-905a-4b5f-9903-0f4459e73ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: model1\n",
      "Model type: simple\n",
      "Result: 30\n",
      "\u001b[3;31m>>> Result is NOT better than the benchmark -> Reject the model as it is not optimal\n"
     ]
    }
   ],
   "source": [
    "# Get the model result and compare with the benchmark\n",
    "\n",
    "print(\"Model name: {}\".format(current_model[\"name\"]))\n",
    "print(\"Model type: {}\".format(current_model[\"type\"]))\n",
    "# Load the result of the current model WITH a VARIABLE PATH\n",
    "result = pd.read_csv(current_model[\"path\"]).iloc[0, 0]\n",
    "print(\"Result: {}\".format(result))\n",
    "\n",
    "if result > benchmark:\n",
    "    print(\"\\033[3;32m>>> Result is better than the benchmark -> Accept the model and use it for calculations\")\n",
    "else:\n",
    "    print(\"\\033[3;31m>>> Result is NOT better than the benchmark -> Reject the model as it is not optimal\")\n",
    "    \n",
    "# Some following calculations with the optimal model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef202e-c989-4814-9b2b-f411f464625c",
   "metadata": {},
   "source": [
    "In lines 5 and 6 of the first cell, the paths are stored with other model details. Therefore we can use them as variables when we need them (e.g. line 15 in the first cell and line 6 in the second cell). Now, when the current model is set to \" model1\", everything is automatically adjusted to its details and we do not need to remember different places and change hard coded paths. Also, if the path details need to be changed at some point, we only need to change them in one place and everything else is automatically adjusted and updated. This saves us a lot of time, mistakes and mental capacity not to remember every place where changes are needed, especially, when our code becomes longer.\n",
    "\n",
    "The same process with a minimal change now leads to a completely different but correct decision. The code correctly states that \"model1\" performs worse than the benchmark. Therefore, it is rejected this time. It is now clear that \"model2\" performs much better and we will use it for all subsequent calculations or visualisations. If it is very easy for readers to reproduce, it is much more likely that readers will spot such small but serious errors. This could save academia from written work whose conclusions do not match the actual results, as was the case with the draft of my bachelor thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148825c-5624-4bc0-93bc-e462df68079f",
   "metadata": {},
   "source": [
    "### Solving compatibility chaos with Docker\n",
    "One might think that it would be easy to copy the code and run it from one computer to another, but it turned out to be a real headache. Different operating systems on my local computer — OK I agree, not all of you run both macOS and Linux on your laptop, but this is pretty common for people working in computer science — and the university server caused many compatibility issues and it was very time-consuming to try to solve them. The university server was running on Ubuntu, a Linux distribution, which was not compatible with my macOS-based code editor. Moreover, it did not support Python programming language and all the deep learning algorithm packages I needed in the project in the same way as my macOS computer did.\n",
    "\n",
    "As a remedy, I used Docker containers, which allowed me to create a virtual environment with all the necessary packages and dependencies installed.  This way I could integrate them with different hardware and use the processing power of that hardware. So this allows anyone to reproduce the project on their local computers or servers. That's where Docker containers came in handy, allowing me to create an environment where everything needed to run the code was installed, and integrate on different hardware. To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. Here's the link to the installation guide for Docker in case you want to mess around with it: [macOS](https://docs.docker.com/docker-for-mac/install/) and [Ubuntu](https://docs.docker.com/engine/install/ubuntu/). I found the Docker website very helpful, with a lot of resources and tutorials available. Once Docker was installed, it was very easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues. Not only did Docker containers save me a lot of time and effort, but they also made it easier for others to reproduce my work.\n",
    "\n",
    "### Why does nobody check your code?\n",
    "But even with Docker being implemented, I still faced another challenge: making the verification process of my code easy enough so that it can be done by anyone without the need to possess any particular knowledge in computer sciences. Have you ever wondered when you submit, for example, some work to your boss or a paper to an academic journal, whether the recipients of the work will really check your code? \n",
    "> <big> Why should you expect that those who receive your work (a) want to check your code and that (b) they can do it?</big>\n",
    "\n",
    "It is very likely that those you expect to check your code will not do it. And the reasons might not be bad will, lack of time, or the excess of trust they may show you. Instead, it may be just a technical barrier that prevents them to give a try. This barrier might be real or not, it does not really matter. It is likely that they have experienced many times issues when trying to replicate code from others that over time, they became more reluctant to try it. They might just not believe that they are able to run your code in an efficient manner, which means fast enough, and without having to debug it. They might be wrong, but this is what they may assume.\n",
    "\n",
    "<figure style=\"float:left; width: 288px; height: 216px; margin: 10px 30px 30px 0;\">\n",
    "    <img src=\"i/cafe.jpg\"/>\n",
    "  <figcaption></figcaption>\n",
    "</figure>\n",
    "\n",
    "<p style=\"color: red;\">Therefore, I think that before you expect someone to run your code (and hopefully check it), at least these two criteria (sufficient speed and absence of bugs) should be fulfilled. Computing operation processes can be accelerated by e.g., reducing the size of the datasets or by implementing computationally efficient data processing approaches (e.g., using pytorch). So when you want people to run your code and make sure that they won't give up</p>\n",
    "\n",
    "<blockquote style=\"display:inline-block;\"> \n",
    "    <big> Aim for a running time about the time to make a coffee or tea </big>\n",
    "</blockquote>\n",
    "\n",
    "Of course, if the data needs to be reduced, you won't get the same results as in your original analysis. So this will not lead to reproducibility _sensu stricto_. However, at least your peers can inspect your code (as long as you mention clearly what are the expected results) and give you some feedback which is a step towards reproducibility. Note that bugs can occur for various reasons. Sometimes, it is only because some code you wrote on a Windows machine does not work on a macOS machine. This rarely happens, but it cannot be excluded, so you'd better check any sources of bugs including this. <span style=\"color: red;\">Here's an example of a simple Python code that works on Windows but not on macOS:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb6b45f-3d84-43a4-b2de-fdca88ba15b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temporary directory path is: None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the temporary directory path\n",
    "temp_dir = os.getenv(\"TEMP\")\n",
    "\n",
    "# Print the path\n",
    "print(\"The temporary directory path is:\", temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa8514-57ba-4370-bae8-f66200e2e46e",
   "metadata": {},
   "source": [
    "On Windows, this code will print the path to the temporary directory, which is typically something like `C:\\Users\\UserName\\AppData\\Local\\Temp`. However, on macOS, the `TEMP` environment variable is not set, so the code will raise a _KeyError_ when trying to access it. To make this code work on both Windows and macOS, you can use the following code instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60446864-3743-4927-bc99-8f40acb71066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temporary directory path is: /tmp\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "# Get the temporary directory path\n",
    "temp_dir = tempfile.gettempdir()\n",
    "\n",
    "# Print the path\n",
    "print(\"The temporary directory path is:\", temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c7628-3ed6-4b22-a5b0-ed7058c1b38f",
   "metadata": {},
   "source": [
    "> <big> Without being carefully checked, your code may hide important mistakes which may remain unnoticed, and this can have various potential consequences, more or less dramatic, depending on each individual case. To improve the chances that your code will be read and checked, you can make your code executable in various OS and computationally more efficient.  </big>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0a38e-5a9e-488a-bde3-910db3b5e74d",
   "metadata": {},
   "source": [
    "### Jupyter, King of the gods notebook\n",
    "<figure style=\"float: right; width: 350px; height: 350px; margin: 10px 0 56px 40px;\">\n",
    "  <img src=\"i/jupyter.png\"/>\n",
    "  <figcaption>Credit: Discord software - Midjourney bot</figcaption>\n",
    "</figure>\n",
    "\n",
    "How to make the verification process of your code feasible and as simple as possible? In my case, I wanted to make sure that my supervisors can appreciate the work I have done for my undergraduate thesis. I spent many efforts to make my code readable, efficient, and also absent of bugs (or at least this is what I was hoping for). The only way for me to ensure this was to facilitate their work by reducing all possible barriers to reproducibility. I found that Jupyter Notebooks was a good choice to increase the chance that they will check my code. This web application allows you to edit your code in your preferred browser and access it from anywhere without any installation required. As long as you have an internet connection you are fine. This text is written in a Jupyter Notebook and is indeed very convenient. You can find the Jupyter Notebook that showcases our work (including the code and results) via this [link](http://c100-159.cloud.gwdg.de:9009/doc/tree/notebooks/main.ipynb). \n",
    "\n",
    "    \n",
    "Jupyter Notebooks combines Markdown text, code, and visualizations, and therefore I could create a complete narrative of my work that was easy to understand and follow. Because you can organize the folder and all files, including all code, results, and visualizations in one location, it is easy for your supervisor (or anyone interested in your work) to quickly understand the processes behind the work that has been produced. Also, it can handle large datasets. Another important aspect is its accessibility. Jupyter Notebooks can be hosted on a cloud-based platform such as GitHub. If so, the work can be easily shared using a simple link. Readers can see exactly what I did, how I did it, and what my results were. In Markdown, it is easy to format text. You can create header and sub-headers using `#` symbols as follows:\n",
    "\n",
    "`# My header ` will produce \n",
    "\n",
    "# My header\n",
    "\n",
    "`## My sub-header ` will produce\n",
    "\n",
    "## My sub-header\n",
    "\n",
    "To write text in italic, you only need to add `_` before and after the corresponding words like this:\n",
    "\n",
    "`_my coffee_` will produce _my coffee_\n",
    "\n",
    "Overall, this allows you to write and format text very easily and therefore save a lot of time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98990d-4c11-4e3e-b65f-68002b178e64",
   "metadata": {},
   "source": [
    "Another important note is that currently, Jupyter Notebooks are free for everyone. This is a huge advantage because it means that anyone can start using them, regardless of their technical skills or budget. Whether you are a beginner or an experienced data scientist, Jupyter Notebooks can help you make your work accessible and reproducible. Jupyter Notebooks allowed me to make various experiments on my code and iterate quickly. Because I can combine text, code, and visualizations, I was able to easily try different approaches to solving a problem and see the results almost in real time. I had the opportunity to test different ideas and get feedback from others without having to spend a lot of time writing and debugging code. I would like to invite you to try Jupyter Notebooks especially if you want to collaborate with others. By sharing your notebook, you can get feedback and see changes made by your collaborators on the projects in real time. \n",
    "\n",
    "### Reproducibility barriers are more common than you may think\n",
    "The problems I faced and that you are likely to have experienced too, are representative of a much global issue that has caused a lot of concern in the scientific community. It is often referred to as the \"reproducibility crisis\" in science$^{1, 2}$. Researchers need to be able to reproduce and replicate findings to make sure that scientific progress is actually happening$^{3-5}$. An important study found that around one-third of social science studies published in top journals like *Nature* and *Science* between 2010 and 2015 couldn't be reproduced$^4$. It does not necessarily mean that research that cannot be reproduced will automatically lead to wrong results. However, if they cannot be reproduced it means that one cannot be sure about the validity of their findings. Given the very high publication standards of these journals, the overall proportion of non-reproducible scientific work that is published is likely to be higher. This is indeed pretty much alarming! \n",
    "\n",
    "It is important to get a common understanding of what \"replicability\" and \"reproducibility\" mean in our context. Here, we use the definitions suggested by the US National Science Foundation (2015)$^6$. \n",
    "> <big> **Reproducibility** means the ability of duplicating the results **using the same materials** as the original investigator.<br>  **Replicability** means being able to duplicate the results of a study by following the same procedures but **using new data**. </big>\n",
    "\n",
    "Reproducibility does not only aim at verifying whether the processes that lead to the results are correct; it is also a first step that is usually required before checking further whether some findings may be generalized to other datasets or contexts. Indeed, before trying to replicate a study one needs at least to be able to reproduce the work using the same materials (data, code, etc.). So reproducibility, which is the focus of our work here, is extremely important and a fundamental pillar of science.\n",
    "\n",
    "### Shouldn't reproducibility be easy?\n",
    "<figure style=\"float: left; width: 475px; height: 475px; margin: 10px 40px 56px 0;\">\n",
    "  <img src=\"i/DL_bkg.png\"/>\n",
    "  <figcaption>Credit: Discord software - Midjourney bot</figcaption>\n",
    "</figure>\n",
    "\n",
    "You might think that getting identical results should be easy with a given dataset and clear methods. In principle, you are right. If the data is available and the procedure to reach the conclusions is clear and well-detailed, there should not be any reasons for others to fail to reproduce the results. Well, this can be true for studies using standard statistical procedures applied to datasets where all operations are carried out within statistical software. But is it true in general that all scientific work is and should be reproducible?\n",
    "\n",
    "You have certainly already observed in your daily life that even by following exactly the same procedure in doing things, some differences in the outputs may occur and you may not be sure about what caused those differences. This happens frequently in cooking. If you try to cook bread from scratch by mixing flour with water, and salt and adding some rising agents, tiny changes in the room temperature or the quality of the flour may require you to adjust the quantity of the ingredients. Otherwise, your bread may look different from a previous attempt, and in a worst-case scenario, may not rise at all.\n",
    "\n",
    "This is somewhat similar when we follow a recipe (or a procedure) that describes the steps required to apply an artificial intelligence (AI) algorithm to some data. If you look at the four images (_left_), their faces look very similar. However, if you look carefully, you can see important differences. Look again, more carefully, and you will see that they are actually very different! These images are the outcome of deep learning algorithms that generate images using the exact same order from the user (a few keywords). Tiny randomness in the algorithms leads to variations in the generated images. Why should it matter to us? When similar algorithms are applied in a context where evidence-based decisions may have potential social, economic, or environmental repercussions (for example, computer simulations that guide policymakers in epidemiology), changes in the results can lead to wrong conclusions, and ill-advised decisions may have catastrophic consequences. Reproducibility is therefore crucial in some areas and making it feasible requires sound design and careful implementation of various procedures. Below you will learn a few tips to ensure that your work can be reproduced even when dealing with very complex algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95553e60-3816-4c46-a4f2-4b5119e071e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Version control with Git and GitHub\n",
    "This has certainly happened to you several times: you lost your work because you forgot to save it, or you erased some files and lost track of their previous version and cannot find them anymore. Or perhaps worse, you lost an entire folder from a failed hard disc without having copies that you might have saved on a secured online storage platform. Keeping a history of track of changes and copies of your work is indeed very important. Also, you might need to collaborate with others, and teamwork should go without the risk of losing any work. This is where version control comes in.\n",
    "\n",
    "[Git](https://git-scm.com/) is a version control system that allows you to keep track of changes to your code over time. [GitHub](https://github.com/) is a web-based platform that provides a central repository for storing and sharing code. With Git and GitHub, I was able to version my code, collaborate with others, and I avoided the risk of losing any work. I really couldn't afford to spend a full year on my dissertation and take the risk of losing it. \n",
    "\n",
    "Git and GitHub are also great for reproducibility. By sharing your code via these platforms, others can access your work, verify it and reproduce your results without risking changing, or worse, destroying your work completely or partially. It also makes it easy for others to build on your work if they want to further develop your research. You can also use Git and GitHub to share or promote your results across a wider community. The ability to easily store and share your code also makes it easier to keep track of the different versions of your code and to see how your work has evolved. You should definitely give it a try! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ff925-2269-4173-a116-43e8fbef2418",
   "metadata": {},
   "source": [
    "### Watch out, randomness is everywhere!\n",
    "\n",
    "One of the challenges with machine learning and deep learning algorithms is that their results can be influenced by their inherent randomness. This means that even if you run the same code multiple times, you may get different results. While this is not necessarily a bad thing &mdash; randomness contributes to mitigating overfitting and generalising the predictions &mdash; it also represents an additional barrier to reproducibility. If you cannot get the same results using the same materials, then you might have good reasons to not trust the findings or at least emit some doubts about their validity. There are many elements of your analysis in which randomness may be present and lead to different results. For example, in a classification (your dependent variable is binary, e.g., success/failure with 1 and 0) or regression (your dependent variable is continuous, e.g., temperature measurements 10.1°C, 2.8°C,...) setting you might need to split your data into training and testing sets. The training set is used to estimate the model (-hyper) parameters and the testing set is used to compute the performance of the model. The way split is usually operationalized as a random selection of rows of your data. So in principle, each time you split your data into training and testing sets, you may end up with different rows in each set. Differences in the training set may lead to different values of the model (-hyper) parameters and affect the predictive performance that is measured from the testing tests. Also, differences in the testing tests may lead to variations in the predictive performance scores and potentially lead to different interpretations, and ultimately, decisions if the results are used for that purpose.\n",
    "\n",
    "But this aspect of randomness in the data splitting procedure used to make statistical inferences based on models, including machine learning and deep learning algorithms, is relatively well known. Randomness may hide in other parts of the code, however. In the Python code below we illustrate a case. We set the _seed number_ to 0 using `np.random.seed(seed value)`. The `random.seed()` function from the package `numpy` (abbreviated `np`) saves the state of a random function so that it can create identical random numbers independently of the machine you use, and this is for any number of executions. The argument of the function in parentheses (here `seed value`) set the previous value number generated by the random number generator. Without providing this seed value, the first execution of the function uses the current system time (this is how the function is implemented but other approaches could be used too). In the example below, we generate two random arrays `arr1` and `arr2` using `np.random.rand(3,2)`. Note that the values `3,2` indicate that we want random values for an array that has `3` rows and `2` columns.\n",
    "\n",
    "![SegmentLocal](i/randomisation-with-seed.gif \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a5c6044-9f1e-4c8b-9838-53ea24a3eb9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Set the seed number e.g. to 0\n",
    "np.random.seed(0)\n",
    "# Generate random array\n",
    "arr1 = np.random.rand(3,2)\n",
    "## print(\"Array 1:\")\n",
    "## print(arr1)\n",
    "\n",
    "#Set the seed number as before to get the same results\n",
    "np.random.seed(0)\n",
    "# Generate another random array\n",
    "arr2 = np.random.rand(3,2)\n",
    "## print(\"\\nArray 2:\")\n",
    "## print(arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c965a-d864-4b77-a13f-f9b1cd41698e",
   "metadata": {},
   "source": [
    "As you can see, if you run it multiple times, the values of `arr1` and `arr2` should remain identical. If this is not the case, check that the `seed value` is set `0` in lines 4 and 11. These identical results are possible because we set the `seed value` to `0`, which ensures that the random number generator produces the same sequence of numbers each time the code is run. Now, let's look at what happens if we remove the line `np.random.seed(0)`:\n",
    "\n",
    "![SegmentLocal](i/randomisation-without-seed.gif \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0542b7-e17b-4b22-bec7-f3c859ab5627",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Generate random array\n",
    "arr1 = np.random.rand(3,2)\n",
    "## print(\"Array 1:\")\n",
    "## print(arr1)\n",
    "\n",
    "#Generate another random array\n",
    "arr2 = np.random.rand(3,2)\n",
    "## print(\"\\nArray 2:\")\n",
    "## print(arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778d61b-2a07-481b-81e0-3da1e148f44c",
   "metadata": {},
   "source": [
    "Here, the values of `arr1` and `arr2` will be different each time we run the code since the `seed value` was not set, and therefore changing over time. This short code demonstrated how randomness that can be controlled by the `seed value` may affect your code. Therefore, unless randomness is required, e.g., to get some uncertainty in the results, setting the `seed value` will contribute to get your work reproducible. I also find it helpful to document the `seed number` I used in my code so that I can easily reproduce my findings in the future. If you are currently working on some code that involves random number generators, it might be worth checking your code and making all necessary changes. In our work (see Code chunk 9 in the [Jupyter notebook](http://c100-159.cloud.gwdg.de:9009/lab/tree/notebooks/main.ipynb?token=7cf55c2887d81e8ea8da627112d0753e4b4fc79345f121fc)) we set the `seed value` in a general way, using a framework (config) so that our code always uses the same seed to train our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c89b78-c724-4fb7-aa3b-c3edd27529d7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I hope you've enjoyed joining us in our quest for reproducibility. We have shown why reproducibility matters and provided a few tips about how to reach it. We have introduced a few important issues that you are likely to encounter during your path to make your work reproducible. In sum we have mentioned:\n",
    "\n",
    "- The need for version control using for example Git and Github, which allows you to keep track of changes in your code and collaborate with others efficiently\n",
    "- Operating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment\n",
    "- The convenience of Jupyter Notebooks for code editing, which is particularly useful for data science and work using deep learning because of its ability to include text and code in the same document and make the work accessible to everyone as long as they have an internet connection\n",
    "- The importance of setting the seed values in random number generators to minimize the risk of barriers to reproducibility\n",
    "-  Some studies cannot be reproduced due to various factors. Therefore, what we have learned cannot be applied to every work. However, when applicable, we believe that reproducibility can be achieved by using the right tools.\n",
    "\n",
    "To summarize all elements that are associated with the reproducibility of our study, we have implemented a system (illustrated in the figure below) composed of four main components (A - D). We use (A) the version control system Git and its hosting service GitHub, which enables a team to share code with peers, efficiently track and synchronize code changes between local and server machines, and reset the project to a working state in case of breaking changes. (B) Docker containers include all necessary objects (engine, data, and scripts). Docker needs to be installed (plain-line arrows) by all users (project leader, collaborator(s), reviewer(s), and public user(s)) on their local machines (C); and (D) we use a user-friendly interface (JupyterLab) deployed from a local machine to facilitate the operations required to reproduce the work. The project leader and collaborators can edit (upload/download) the project files stored on the GitHub server (plain-line arrows) while reviewers and public users can only read the files (dotted-line arrows). \n",
    "\n",
    "![Figure 1](../data/workflow/docker-workflow.jpg \"Figure 1\")\n",
    "\n",
    "Now, let's take a moment to reflect on why reproducibility is so crucial in work using deep learning algorithms. In practice, these algorithms can be used to make decisions that affect people's lives. They may be used to make (or help make) medical diagnoses, financial predictions, and criminal justice assessments. In such cases, it is essential that the results are reliable, and the methods to obtain them are verifiable. Otherwise, hidden errors could lead to wrong conclusions and have serious consequences. Reproducibility is also crucial in research. By making your work accessible and verifiable, you can build on the efforts of others and make advances in your field more efficiently. It also helps mitigate the risk of duplicated work, which may save time and resources.\n",
    "\n",
    "In conclusion, we would like to emphasize that reproducibility is crucial and must be upheld in all scientific endeavours, as long as it remains feasible. Having the right tools and processes in place can ensure that your work is verifiable, which is a necessary (though not sufficient) step in achieving reproducibility. Whether you are working in a research environment or an industry setting, these principles are essential for advancing the field and building trust between modellers and our society. We now encourage you to follow the [2. GitHub readme: Instructions](https://github.com/dsvanidze/replicability#making-deep-learning-algorithms-reproducible-the-devil-is-in-the-details) if you want to implement all tools required to make your work reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d79e2-6111-41a8-983a-ff0b2c576295",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca7ac5-0347-4c99-a78a-b34e6dbc427e",
   "metadata": {},
   "source": [
    "1. Roger D. Peng. Reproducible research in computational science. _Science_, 334(6060): 1226–1227, 2011.\n",
    "2. John P. A. Ioannidis, Sander Greenland, Mark A Hlatky, Muin J Khoury, Malcolm R Macleod, David Moher, Kenneth F Schulz, and Robert Tibshirani. Increasing value and reducing waste in research design, conduct, and analysis. _The Lancet_, 383(9912):166–175, 2014.\n",
    "3. Open Science Collaboration. Estimating the reproducibility of psychological science. _Science_, 349(6251):aac4716, 2015.\n",
    "4. Colin F. Camerer, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, Gideon Nave, Brian A Nosek, Thomas Pfeiffer, et al. Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. _Nature Human Behaviour_, 2018.\n",
    "5. Monya Baker. Reproducibility crisis. _Nature_, 2016.\n",
    "6. Kenneth Bollen, J.T. Cacioppo, R.M. Kaplan, J.A. Krosnick, and J.L. Olds. Social, behavioral, and economic sciences perspectives on robust and reliable science: Report of the Subcommittee on Replicability in Science, Advisory Committee to the US National Science Foundation Directorate for Social, Behavioral, and Economic Sciences. US National Science Foundation, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a51bf9-3c38-4708-812d-6e90cb0a1ec5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .jp-MarkdownCell .jp-RenderedHTMLCommon p {\n",
       "        text-align: justify;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<style>\n",
    "    .jp-MarkdownCell .jp-RenderedHTMLCommon p {\n",
    "        text-align: justify;\n",
    "    }\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
